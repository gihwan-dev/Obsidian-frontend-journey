# 📌 핵심 한 줄 요약

> 컴퓨터는 모든 실수를 정확히 저장하지 못한다.  
> 그래서 어떤 순간부터는 “더 정밀하게” 계산하는 게 불가능해진다.

---

# 🧠 1️⃣ 왜 그런 일이 생기나?

컴퓨터는 숫자를 **무한 정밀도**로 저장하지 못한다.

자바스크립트는 숫자를  
👉 **IEEE 754 64-bit 부동소수점(double precision)** 으로 저장한다.

이건 대략:

- 약 **15~16자리 십진수 정밀도**
- 그 이상은 잘려 나감

---

# 🔍 2️⃣ 큰 수에서 무슨 일이 생기나?

예를 들어 보자:

```js
1000000000000000  // 10^15
```

이 숫자는 이미 정밀도의 거의 끝자락이다.

이 상태에서 아주 작은 변화를 더해보면:

```js
1000000000000000 + 0.001
```

이 값은 실제로는 바뀌지 않는다.

왜냐하면:

> 10^15 규모에서 0.001은 너무 작아서  
> 표현 가능한 비트 단위 변화보다 작기 때문.

즉,

```
1000000000000000
1000000000000000.001
```

이 둘을 컴퓨터는 **같은 숫자**로 본다.

---

# 🔄 3️⃣ 뉴턴 방법에서 무슨 일이 벌어지나?

뉴턴 반복은 점점 조금씩 값을 수정한다:

```
guess → 조금 더 정확한 guess → 또 조금 더 정확한 guess
```

그런데 어느 순간:

```
새로운 guess === 이전 guess
```

이 되어버린다.

왜?

- 계산은 계속 하지만
    
- 저장 가능한 정밀도 한계에 도달했기 때문
    

즉,

> 더 정확한 값이 나와도  
> 저장할 수 없어서 값이 더 이상 변하지 않는다.

---

# 📉 4️⃣ 그래서 왜 문제가 되나?

기존 종료 조건은:

```js
abs(square(guess) - x) < 0.001
```

인데,

- guess가 더 이상 변하지 않음
- 오차는 여전히 0.001보다 큼
- 그런데 더 개선할 방법도 없음

👉 그래서 무한 반복 가능성 발생

---

# 🧊 직관적 비유

생각해봐:

- 1km 자로 나노미터를 재려고 하는 것과 같다.
    
- 도구의 해상도가 부족하다.
    

또는:

> 돋보기로 세균을 보려는 것과 같다.  
> 더 확대해도 안 보이는 건 해상도 한계 때문.

---

# 📌 수학 vs 컴퓨터

수학에서는:

$$
\sqrt{x}  
$$

는 정확히 존재한다.

하지만 컴퓨터에서는:

$$
\sqrt{x} \approx \text{근삿값}  
$$

만 가능하다.

그리고 그 근삿값도  
**표현 가능한 비트 한계 안에서만** 존재한다.

---

# 🎯 그래서 1.7의 핵심은

- 수학은 연속적이다.
- 컴퓨터는 이산적이다.
- 알고리즘은 하드웨어 현실을 고려해야 한다.

---

# 1️⃣ IEEE 754 구조 (Double Precision, 64-bit)

자바스크립트 숫자는 64비트로 이렇게 생겼다:

```
| S |   Exponent (11 bits)   |         Fraction (52 bits)        |
|---|------------------------|-----------------------------------|
| 1 |          11            |               52                  |
```

### 의미

숫자는 이렇게 표현된다:

$$
(-1)^S \times 1.Fraction \times 2^{Exponent - Bias}  
$$

- **S** : 부호 (0 = 양수, 1 = 음수)
- **Exponent** : 지수 (크기를 결정)
- **Fraction (Mantissa)** : 유효숫자 부분 (정밀도 담당)

---

# 2️⃣ 왜 큰 수일수록 작은 차이를 못 표현하나?

핵심은 이것이다:

> 정밀도는 Fraction(52비트)에 고정되어 있다.

즉, 유효숫자는 항상 약 52비트 ≈ 15~16자리 십진수 정도.

---

## 🔍 작은 수 예시

$$
1.000000000000000  
$$

여기서

$$
1 + 0.000000000000001  
$$

은 표현 가능하다.

왜?

- 유효숫자 영역 안에 있기 때문.

---

## 🔥 큰 수 예시

$$
10^{15} = 1000000000000000  
$$

이 숫자는 이미 16자리다.

이 상태에서:

$$
10^{15} + 0.001  
$$

을 더하면?

컴퓨터는 이렇게 본다:

```
1000000000000000.000000
```

왜냐하면:

- 0.001은 이미 표현 가능한 최소 단위보다 작다.
- 저장할 비트 공간이 없다.

---

# 📌 더 정확히 말하면

부동소수점에서 **표현 가능한 간격(ULP)** 은 크기에 따라 달라진다.

> 참고: [[ULP란?]]

$$
\text{간격} \propto 2^{Exponent}  
$$

즉,

- 수가 커질수록
- 표현 가능한 숫자 사이 간격이 커진다.

작은 수 근처:

```
1.000000000000001
1.000000000000002
```

큰 수 근처:

```
1000000000000000
1000000000000002
```

중간 값은 존재하지 않는다.

---

# 3️⃣ 뉴턴 방법에서 왜 문제가 되나?

뉴턴 반복:

$$
y_{n+1} = \frac{y_n + \frac{x}{y_n}}{2}  
$$

수렴이 거의 끝나면:

$$
y_{n+1} - y_n  
$$

이 아주 작아진다.

그런데 그 차이가 표현 가능한 최소 단위보다 작으면:

$$
y_{n+1} = y_n  
$$

이 되어버린다.

더 이상 개선이 불가능하다.

---

# 4️⃣ 왜 절대 오차가 깨지나?

절대 오차 조건:

$$
|y^2 - x| < 0.001  
$$

문제:

- 큰 수에서 0.001은 “아주 작은 기준”
- 작은 수에서 0.001은 “엄청 큰 기준”

즉, 기준이 스케일에 비례하지 않는다.

---

# 5️⃣ 왜 상대 오차가 해결책인가?

상대 오차:

$$
\frac{|y^2 - x|}{x} < \varepsilon  
$$

이건 의미가 이렇게 바뀐다:

> 전체 값에 비해 얼마나 틀렸는가?

예:

| x      | 절대 오차 0.001 의미 | 상대 오차 의미 |
| ------ | -------------- | -------- |
| 1      | 0.1% 오차        | 0.1%     |
| 10^15  | 거의 0%          | 0.1%     |
| 0.0001 | 1000% 오차       | 0.1%     |

상대 오차는 항상 **비율 기준**이다.

---

# 🔥 핵심 직관

절대 오차는 “센티미터 기준”  
상대 오차는 “비율 기준”

큰 건물에 대해 1cm 오차는 무의미.  
작은 나사에 대해 1cm 오차는 치명적.

상대 오차는 크기에 맞춰 자동 조정된다.

---

# 6️⃣ 더 깊은 이유

부동소수점은 본질적으로:

$$
\text{유효숫자} \times 2^{지수}  
$$

형태다.

즉, 정밀도는 항상 **비율적(relative)** 이다.

따라서 종료 조건도 비율적으로 해야 논리적으로 일관된다.

---

# 🎯 SICP가 여기서 말하는 진짜 교훈

1. 수학은 연속적이다.
2. 컴퓨터는 이산적이다.
3. 정밀도는 유한하다.
4. 알고리즘은 표현 체계를 고려해야 한다.

---