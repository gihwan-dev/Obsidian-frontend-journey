> 원문: https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f

Large Language Models 덕분에 인공 지능은 이제 거의 모든 사람의 관심을 끌고 있다. 가장 유명한 LLM인 ChatGPT는 자연어가 매우 자연스러운 인터페이스이기 때문에 즉시 인기를 끌었고, 이로 인해 모두가 인공지능을 쉽게 사용할 수 있게 되었다. 그럼에도 불구하고, LLM이 어떻게 동작하는지는 데이터 과학자나 AI 관련 직종에 있지 않다면 여전히 잘 알려져 있지 않다. 이 글에서는 그 부분을 바꿔보려고 한다.

솔직히 말해, 그건 야심찬 목표다. 결국, 오늘날 우리가 가진 강력한 LLM들은 수십 년에 걸친 AI 연구의 결실이다. 안타깝게도, 이를 다루는 대부분의 글들은 두 가지 유형 중 하나이다. 매우 기술적이어서 많은 사전 지식을 요구하거나, 너무 단순해서 읽고 나서도 알게 되는 것이 없다.

이 글은 두 가지 접근 방식 사이의 균형을 맞추기 위한 것이다. 아니, 좀 더 정확히 말하면, 완전히 처음부터 LLM이 어떻게 훈련되고 왜 그렇게 잘 작동하는지 까지 안내하는 것이 목적이다. 우리는 그 과정에서 관련된 모든 핵심 요소들만 짚어가며 설명할 것이다.

이 글은 모든 세부적인 내용을 깊이 파고들지는 않을 것이므로, 수학보다는 직관에, 그리고 가능한 한 시각 자료에 의존할 것이다. 하지만 보다시피, 세부적으로는 매우 복잡한 주제이지만, LLM의 주요 메커니즘은 매우 직관적이며, 그것만으로도 충분할 수 있다.

이 글은 또한 ChatGPT와 같은 LLM을 더 잘 활용하는 데에도 도움이 될 것이다. 실제로, 유용한 답변을 얻을 확률을 높일 수 있는 몇 가지 유용한 팁도 배울 것이다. 유명한 AI 연구자이자 엔지니어인 Andrei Karparthy가 최근에 강조해서 말했듯이, "영어는 가장 핫한 새로운 프로그래밍 언어다"라고 할 수 있다.

하지만 먼저, LLM이 인공지능의 세계에서 어디에 위치하는지부터 이해해보자.

![[Pasted image 20250803171747.png]]

AI 분야는 종종 여러 계층으로 시각화된다:

- **Artificial Intelligence (AI)**: 매우 광범위한 용어이지만, 일반적으로 지능적인 기계를 의미한다.
- **Machine Learning (ML)**: AI의 하위 분야로, 데이터에서 패턴을 인식하는 데 중점을 둔다. 패턴을 한 번 인식하면, 그 패턴을 새로운 관찰에 적용할 수 있다. 이것이 핵심 개념이며, 곧 자세히 다루겠다.
- **Deep Learning**: ML 내에서 비정형 데이터(텍스트와 이미지)에 집중하는 분야다. 이는 인공 신경망에 기반하며, 이는 두뇌에서 영감을 받은 방법이다.
- **Large Language Models (LLMs)**: 특히 텍스트를 다루며, 이 글의 주요 주제가 될 것이다.

이제 각 계층에서 관련된 핵심 요소들만을 짚어가며 설명할 것이다. 가장 바깥쪽인 인공지능(AI)은 너무 일반적이므로 건너뛰고, 바로 머신러닝부터 시작하겠다.

![[Pasted image 20250803172107.png]]

머신러닝의 목표는 데이터에서 패턴을 발견하는 것이다. 좀 더 구체적으로 말하면, 입력과 결과 사이의 관계를 설명하는 패턴을 찾는 것이다. 이것은 예시를 통해 가장 잘 설명할 수 있다.

예를 들어, 두 가지 음악 장르인 레게톤과 R&B를 구분하고 싶다고 해보자. 만약 이 장르들이 익숙하지 않다면, 이 작업을 이해하는 데 도움이 될 아주 간단한 소개를 하겠다. 레게톤은 경쾌한 비트와 춤추기 좋은 리듬으로 유명한 라틴 어반 장르이고, R&B(리듬 앤 블루스)는 아프리카계 미국인의 음악 전통에 뿌리를 둔 장르로, 감성적인 보컬과 빠른 곡과 느린 곡이 혼합된 것이 특징이다.

![[Pasted image 20250803172310.png]]

예를 들어, 20곡이 있다고 가정해보자. 우리는 각 곡의 템포와 에너지라는 두 가지 지표를 알 고 있다. 이 두 지표는 어떤 곡이든 간단히 측정하거나 계산할 수 있다. 또한, 각 곡에 레게톤 또는 R&B라는 장르 라벨을 붙여두었다. 데이터를 시각화해보면, 에너지와 템포가 높은 곡들은 주로 레게톤이고, 템포와 에너지가 낮은 곡들은 대부분 R&B 임을 알 수 있다. 이는 자연스러운 결과다.

하지만 매번 직접 장르를 라벨링하는 것은 시간이 많은 들고 확장성이 떨어진다. 대신, 곡의 지표(템포, 에너지)와 장르 사이의 관계를 학습해서, 쉽게 얻을 수 있는 지표만으로 장르를 예측할 수 있다.

머신러닝 용어로 이를 분류 문제(classification problem)라고 한다. 결과 변수(장르)가 고정된 클래스/라벨(여기서는 레게톤과 R&B) 중 하나만 가질 수 있기 때문이다. 반면, 결과가 연속적인 값(예: 온도나 거리)인 경우는 회귀 문제(regression problem)이라고 한다.

우리는 이제 장르가 이미 라벨링된 데이터셋을 사용해 머신러닝 모델을 "학습(train)" 시킬 수 있다. 시각적으로 보면, 모델을 학습시키는 과정은 두 클래스를 가장 잘 구분하는 선을 찾는 것이다.

이것이 왜 유용할까? 이제 선을 알게 되었으니, 새로운 곡이 들어왔을 때 그 곡이 선의 어느 쪽에 위치하는지에 따라 레게톤인지 R&B인지 예측할 수 있다. 필요한 것은 템포와 에너지뿐이므로, 사람이 일일이 장르를 지정하는 것보다 훨씬 간단하고 확장성도 뛰어나다.

또한, 선에서 멀리 떨어질수록 예측이 더 확실하다고 볼 수 있다. 따라서 선에서의 거리를 기준으로 예측의 신뢰도를 표현할 수도 있다. 예를 들어, 새로 들어온 저 에너지, 저 템포 곡에 대해 98% 확률로 R&B라고, 2% 확률로 레게톤이라고 말할 수 있다.

![[Pasted image 20250803173229.png]]

물론 현실은 그보다 훨씬 더 복잡하다.

클래스를 구분하는 가장 좋은 경계가 반드시 직선일 필요는 없다. 즉, 입력과 결과 사이의 관계가 더 복잡할 수 있다. 위 그림처럼 곡선일 수도 있고, 그보다 훨씬 더 복잡할 수도 있다.

현실은 또 다른 면에서도 더 복잡하다. 예시처럼 입력 변수가 두 개뿐인 경우는 드물고, 실제로는 수십, 수백, 심지어 수천 개의 입력 변수가 있을 수 있다. 또한, 두 개 이상의 클래스가 존재하는 경우가 많다. 모든 클래스가 이 많은 입력 변수들과 매우 복잡하고 비선형적인 관계를 맺고 있을 수 있다.

우리의 예시에서도 실제로는 두 개 이상의 장르가 있고, 템포와 에너지 외에도 더 많은 지표가 필요하다는 것을 알고 있다. 이들 사이의 관계도 아마 그렇게 단순하지 않을 것이다.

여기서 꼭 기억해야 할 점은 이것이다: 입력과 출력 사이의 관계가 복잡해질수록, 그 관계를 학습하기 위해서는 더 복잡하고 강력한 머신러닝 모델이 필요하다는 것이다. 보통 입력 변수와 클래스의 수가 많아질수록 복잡성도 증가한다.

그리고 그에 더해, 더 많은 데이터도 필요하다. 왜 이것이 중요한지는 곧 알게 될 것이다.

![[Pasted image 20250803173655.png]]

이제 약간 다른 문제로 넘어가 보겠다. 이번에도 앞서 사용한 멘탈 모델을 적용해볼 것이다. 이번 문제에서는 입력값으로 이미지를 사용한다. 예를 들어, 가방에 들어 있는 귀여운 고양이 사진 같은 이미지다.

결과값으로는 이번에는 세 가지 라벨, 즉 호랑이, 고양이, 여우 중 하나가 있다고 가정해보자. 양 떼를 보호하기 위해 호랑이가 보이면 경보를 울리고, 고양이나 여우가 보이면 경보를 울리지 않아야 하는 상황이다.

이 역시 출력값이 몇 가지 고정된 클래스 중 하나만 가질 수 있으므로 분류 문제임을 이미 알고 있다. 따라서 이전과 마찬가지로, 라벨이 지정된 데이터(즉, 클래스 라벨이 할당된 이미지)를 사용해 머신러닝 모델을 학습시킬 수 있다.

하지만 시각적 입력을 어떻게 처리할지는 명확하지 않다. 컴퓨터는 오직 숫자 입력만 처리할 수 있기 때문이다. 앞서 곡의 에너지와 템포는 숫자였다. 다행히 이미지도 픽셀로 이루어진 숫자 입력이다. 이미지에는 높이, 너비, 그리고 세 개의 채널(빨강, 초록, 파랑)이 있다. 이론적으로는 픽셀 값을 머신러닝 모델에 바로 입력할 수 있다(여기서 공간적 요소는 일단 무시하겠다).

하지만 여기서 두 가지 문제가 생긴다. 첫째, 작은 저화질 224x224 이미지조차 15만 개가 넘는 픽셀(224x224x3)로 이루어져 있다. 앞서 입력 변수가 많아야 수백 개, 많아도 천 개 정도라고 했는데, 이제는 갑자기 최소 15만 개가 된 것이다.

둘째, 원시 픽셀과 클래스 라벨 사이의 관계를 생각해보면, 머신러닝 관점에서 그 관계는 매우 복잡하다. 인간의 뇌는 호랑이, 여우, 고양이를 쉽게 구분할 수 있지만, 15만 개의 픽셀을 하나씩 본다면 이미지에 무엇이 있는지 전혀 알 수 없다. 하지만 머신러닝 모델은 바로 이렇게 픽셀을 보기 때문에, 원시 픽셀과 이미지 라벨 사이의 관계를 처음부터 학습해야 하며, 이는 결코 쉬운 일이 아니다.

![[Pasted image 20250803183821.png]]

이번에는 또 다른 유형의 입력-출력 관계, 즉 문장과 그 감정(감성) 사이의 매우 복잡한 관계를 생각해보자. 여기서 감정이란, 문장이 전달하는 감정(예를 들어 긍정 또는 부정)을 의미한다.

문제 설정을 다시 정리해보면, 입력값은 단어들의 시퀀스, 즉 문장이고, 결과값은 감정(긍정 또는 부정)이다. 앞서와 마찬가지로, 이것도 두 가지 라벨(긍정, 부정) 중 하나를 예측하는 분류 문제이다.

이미지 예시에서처럼, 인간은 이 관계를 자연스럽게 이해하지만, 머신러닝 모델도 같은 일을 할 수 있도록 가르칠 수 있을까?

그 전에, 단어를 머신러닝 모델이 처리할 수 있는 숫자 입력으로 바꾸는 방법이 처음에는 명확하지 않다. 사실, 이미지는 본질적으로 이미 숫자이기 때문에 더 쉬웠지만, 단어는 그렇지 않다. 여기서 자세히 다루지는 않겠지만, 알아두어야 할 점은 모든 단어가 '워드 임베딩'이라는 형태로 변환될 수 있다는 것이다.

간단히 말해, 워드 임베딩은 단어의 의미와 문법적 특성을(종종 특정 맥락 내에서) 숫자로 표현한 것이다. 이러한 임베딩은 머신러닝 모델을 학습시키는 과정에서 얻거나, 별도의 학습 절차를 통해 얻을 수 있다. 보통 워드 임베딩은 단어 하나당 수십~ 수천 개의 변수로 이루어져 있다.

정리하자면, 문장을 워드 임베딩이라는 숫자 시퀀스로 바꿀 수 있고, 이 임베딩에는 의미와 문법 정보가 담겨있다. 이렇게 변환된 입력을 머신러닝 모델에 넣을 수 있다. (여기에는 앞선 예시들과 달리 '순서'라는 새로운 차원이 있지만, 여기서는 일단 넘어가겠다.)

하지만 이제는 시각적 입력에서와 마찬가지로, 또 다른 어려움이 생긴다. 긴 문장(혹은 단락, 심지어 문서 전체)의 경우, 워드 임베딩의 크기가 커서 입력 변수의 수가 매우 많아질 수 있다.

두 번째 문제는 언어와 감정 사이의 관계가 매우 복잡하다는 점이다. 예를 들어 "That was a great fall(정말 엄청난 추락이었어)" 같은 문장은 여러 가지로 해석될 수 있다(심지어 비꼬는 의미로도).

이런 문제를 해결하려면 매우 강력한 머신러닝 모델과 많은 데이터가 필요하다. 바로 여기서 딥러닝이 등장한다.

![[Pasted image 20250803191006.png]]

우리는 이미 머신러닝의 기본과 더 강력한 모델이 필요한 이유를 살펴보면서 LLM을 이해하는 데 큰 걸음을 내디뎠다. 이제 딥러닝을 소개하면서 또 한 번 큰 도약을 하게될거다.

입력과 출력 사이의 관계가 매우 복잡하거나, 입력 또는 출력 변수의 수가 많을 때(앞서 본 이미지와 언어 예시 모두 해당됨), 더 유연하고 강력한 모델이 필요하다는 점을 이야기했다. 선형 모델이나 그와 비슷한 단순한 모델로는 이런 시각적 분류나 감정 분류 문제를 해결할 수 없다.

여기서 신경망(Neural Network)이 등장한다.

![[Pasted image 20250803191206.png]]

신경망은 임의의 복잡한 관계를 모델링할 수 있는 강력한 머신러닝 모델이다. 신경망은 이러한 복잡한 관계를 대규모로 학습할 수 있게 해주는 엔진 역학을 한다.

실제로 신경망은 뇌에서 영감을 받았지만, 실제 유사성에 대해서는 논란의 여지가 있다. 신경망의 기본 구조는 비교적 단순하다. 입력 신호가 결과 변수를 예측하기 위해 통과하는 여러 층의 "뉴런"으로 이루어져 있다. 여러 층의 선형 회귀가 쌓여 있고, 그 사이에 비선형성이 추가되어 신경망이 매우 비선형적인 관계도 모델링할 수 있게 해준다.

> [!Note] 단어 노트
> **선형 회귀란?**
> 
> 이 글 앞부분에서 선형 회귀를 이렇게 설명한다:
> 
> “모델이 두 클래스를 구분하는 가장 좋은 선(직선)을 찾는 것”
> 
> 즉, 선형 회귀는 데이터를 나누는 **직선**(혹은 평면)을 찾는 방법이다. 예를 들어, 노래의 템포와 에너지를 기준으로 레게톤과 R&B를 나누는 선을 찾는 것처럼.
> 
> **비선형성이란?**
> 
> 하지만 현실에서는 직선 하나로 모든 데이터를 나누기 어렵다. 예를 들어, 데이터가 곡선이나 복잡한 모양으로 섞여 있다면 직선으로는 구분이 안 된다.  
> 이때 신경망은 각 층마다 “비선형 함수”(예: 꺾이는 함수)를 적용해서, **직선이** **아닌** **복잡한** **경계**도 만들 수 있게 해다.
> 
> 한 줄 요약
> 
> - **선형 회귀:** 데이터를 직선(혹은 평면)으로 나누는 것
> - **비선형성:** 직선만으로 안 될 때, 꺾이거나 굽은 경계도 만들 수 있게 해주는 것

신경망은 여러 층으로 이루어진 경우가 많아(그래서 '딥러닝'이라는 이름이 붙었다) 매우 거대해질 수 있다. 예를 들어, ChatGPT는 약 1,760억 개의 뉴런으로 구성된 신경망을 기반으로 하는데, 이는 인간의 뇌(1,000억개) 보다도 많다.

이제부터는 신경망을 머신러닝 모델로 가정하고, 이미지와 텍스트를 어떻게 처리하는지도 알고 있다는 전제로 설명을 이어가겠다

![[Pasted image 20250803192448.png]]

이제 우리는 대형 언어 모델(LLM)에 대해 이야기할 수 있게 되었고, 이제부터 흥미로운 얘기가 시작된다. 여기까지 잘 따라왔다면, 이제 LLM을 이해할 수 있는 모든 지식을 갖추었다고 볼 수 있다.

어떻게 시작하는게 좋을까? 아마 '대형 언어 모델'이 실제로 무엇을 의미하는지 설명하는 것부터가 좋을것이다. '대형(Large)'이 무엇을 뜻하는지는 이미 알고 있다. 신경망 안에 있는 뉴런(파라미터)의 수를 의미한다. 정확히 몇 개부터 대형 언어 모델이라고 부르는지는 명확하지 않지만, 보통 10억 개 이상의 뉴런이 있으면 '대형'이라고 볼 수 있다.

이제 '언어 모델'이 무엇인지 알아보자. 이 부분은 곧 설명할 예정이고, 조금만 더 가면 ChatGPT의 'GPT'가 무엇을 의미하는지도 알게 될것이다. 하지만 한 번에 하나씩 차근차근 알아보자.

![[Pasted image 20250803195421.png]]

다음과 같은 아이디어를 머신러닝 문제로 바꿔 생각해보자. 주어진 단어들의 순서(즉, 한 문장이나 단락)에서 다음에 올 단어는 무엇일까? 다시 말해, 우리는 다음에 올 단어를 예측하는 방법을 배우고자 하는 것이다. 이 글의 앞부분에서 이미 이런 문제를 머신러닝 문제로 바꿀 수 있는 모든 내용을 배웠다. 사실, 이 작업은 앞에서 다룬 감정 분류와 크게 다르지 않다.

그 예시처럼, 신경망에 입력되는 것은 단어들의 순서(문장)이고, 결과값(출력)은 다음에 올 단어이다. 이것 역시 분류 문제이다. 단지, 이번에는 두 개나 몇 개의 클래스(라벨)만 있은 것이 아니라, 단어의 수만큼 클래스가 있다는 점이 다르다는 것이다 -- 예를 들어 약 5만개 정도가 될 수 있다. 이것이 바로 언어 모델링(language modeling)이다 -- 즉, 다음 단어를 예측하는 법을 배우는 것.

물론, 이 문제는 앞서 본 이진 감정 분류보다 훨씬 더 복잡하다. 하지만 이제 우리는 신경망과 그 강략ㅎㅁ을 알게 되었으니, 이 복잡함에 대한 답은 "안될 이유가 없다" 라고 할 수 있다.

> [!Note] 주의
> 여기서 많은 부분을 단순화해서 설명하고 있다. 실제로는 더 복잡하지만, 주요 원리를 이해하는데 더 방해가 되지 않도록 일부 세부 사항은 생략하고 있다.

![[Pasted image 20250803200004.png]]

우리는 이제 과제가 무엇인지 알고 있으니, 신경망을 학습시키기 위한 데이터가 필요하다. 사실 "다음 단어 예측" 과제를 위한 데이터를 많이 만드는 것은 어렵지 않다. 인터넷, 책, 논문 등에는 텍스트가 아주 풍부하게 존재하고, 이 모든것으로 방대한 데이터셋을 쉽게 만들 수 있다. 심지어 데이터를 따로 라벨링 할 필요도 없다. 왜냐하면 다음에 올 단어 자체가 라벨이기 때문이다. 그래서 이런 방식을 '자기지도 학슴(self-supervised learning)'이라고 부른다.

위의 이미지는 이 과정이 어떻게 이루어지는지 보여준다. 한 문장(시퀀스)만으로도 여러 개의 학습용 시퀀스를 만들 수 있다. 그리고 우리는 이런 쉬컨스를 아주 많이 가지고 있다. 중요한 점은, 짦은 문장부터 수천 단어에 이르는 긴 문장까지 다양한 길이의 시퀀스를 사용해, 모든 상황에서 다음에 올 단어가 무엇인지 학습한다는 것이다.

정리하자면, 우리가 여기서 하는 일은 신경망(LLM)을 훈련시켜 주어진 단어 시퀀스에서 다음에 올 단어를 미리 예측하게 만드는 것이다. 그 시퀀스가 길든 짧든, 독일어든, 영어든, 트윗이든 수식이든, 코드 조각이든 상관없다. 이런 모든 것들이 학습 데이터에 포함된다.

충분히 큰 신경망과 충분한 데이터를 갖추면, LLM은 다음에 올 단어를 매우 잘 예측할 수 있게 된다. 완벽할까? 물론 아니다. 한 시퀀스 뒤에 올 수 있는 단어가 여러 개일 수 있기 때문이다. 하지만 문법적으로나 의미적으로 적절한 단어들 중 하나를 잘 고를 수 있게 된다.

![[Pasted image 20250803202057.png]]

이제 한 단어를 예측할 수 있게 되었으니, 예측한 단어를 다시 LLM에 입력해 또 다른 단어를 예측할 수 있다. 이런 식으로 반복하면, 훈련된 LLM을 이용해 단어 하나만이 아니라 전체 문장을 생성할 수 있다. 그래서 LLM은 '생성형 AI(Generative AI)'의 한 예시가 된다. 즉, LLM에게 한 번에 한 단어씩 말을 하도록 가르친 셈이다.

여기서 중요한 점이 하나 더 있다. 항상 가장 가능성이 높은 단어만 예측할 필요는 없다. 예를 들어, 매번 가장 가능성이 높은 다섯 개의 단어 중에서 하나를 무작위로 선택할 수도 있다. 이렇게 하면 LLM에서 더 창의적인 결과가 나올 수 있다. 실제로 일부 LLM은 결과가 얼마나 결정적(항상 같음)일지, 혹은 창의적(매번 다름)일지 사용자가 선택할 수 있게 해준다. ChatGPT도 이런 샘플링 방식을 사용하기 때문에, 같은 질문을 여러 번 해도 매번 똑같은 답이 나오지 않는 것이다.

그런데 ChatGPT라는 이름에서 'LLM'이 아니라 왜 'GPT'가 들어갔을까? 사실 언어 모델링(language modeling)은 끝이 아니라 시작에 불과하다. 그렇다면 ChatGPT의 'GPT'는 무엇을 의미할까?

![[Pasted image 20250803202446.png]]

사실 우리는 이미 G가 무엇을 의미하는지 배웠다. 바로 "generative(생성)"인데, 이는 앞서 설명한 것처럼 언어 생성 과제로 훈련되었다는 뜻이다. 그렇다면 P와 T는 무엇일까?

T는 여기서 간단히 넘어가겠다. "transformer(트랜스포머)"를 의미하는데, 영화에 나오는 그 트랜스포머가 아니라, 신경망의 한 종류인 트랜스포머 구조를 의미한다. 이 부분은 깊이 신경 쓸 필요는 없지만, 설명하자면 트랜스포머 구조의 가장 큰 장점은 입력 시퀀스(문장)에서 그때그때 가장 중요한 부분에 집중(attention)할 수 있다는 점이다. 인간도 마찬가지로, 어떤 일을 할 때 가장 중요한 것에 집중하고 나머지는 무시한다.

이제 P에 대해 설명하겠다. P는 "pre-trainning(사전 학습)"을 의미한다. 왜 갑자기 '학습(training)'이 아니라 '사전 학습(pre-training)'이라는 말을 쓰는지, 이제 그 이유를 설명하겠다.

그 이유는 ChatGPT 같은 대형 언어 모델이 실제로 여러 단계에 걸쳐 학습되기 때문이다.

![[Pasted image 20250803202819.png]]

### 사전 학습(Pre-training)
첫 번째 단계는 사전 학습으로, 우리가 방금까지 살펴본 과정과 같다. 이 단계에서는 엄청나게 많은 데이터를 사용해 다음에 올 단어를 예측하는 법을 학습한다. 이 과정에서 모델은 언어의 문법과 구문 뿐만 아니라, 세상에 대한 다양한 지식, 그리고 나중에 다룰 몇 가지 새로운 능력까지 습듭하게 된다.

하지만 여기서 몇 가지 질문이 생긴다. 이런 사전 학습 방식에는 어떤 문제가 있을까? 여러 가지가 있겠지만, 여기서 강조하고 싶은 문제는 LLM이 실제로 무엇을 배우는가에 관한 것이다.

즉, LLM은 주어진 주제에 대해 계속해서 말을 이어가는 법을 주로 배운다. 이 작업은 매우 잘할 수 있지만, 우리가 AI에게 기대하는 질문이나 지시 같은 입력에 잘 반응하지는 못한다. 즉, 이 모델은 '도우미(assistant)'로서의 역할을 배우지 않았기 때문에, 실제로도 그렇게 행동하지 않는다.

예를 들어, 사전 학습만 된 LLM에게 "당신의 이름은 무엇인가요?" 라고 물으면, "당신의 성은 무엇인가요?" 라고 답할 수도 있다. 왜냐하면 사전 학습 과정에서 이런 빈 양식(form) 데이터를 많이 봤기 때문이다. 즉, 입력된 문장을 단순히 이어서 완성하려고만 하는 것이다.

이처럼, 명령(지시) 다음에 답변이 나오는 구조는 학습 데이터에서 흔히 볼 수 있는 형태가 아니기 때문에, LLM은 지시를 잘 따르지 못한다. Quora나 StackOverflow 같은 사이트가 그나마 이런 구조에 가까운 데이터를 제공할 수 있다.

이 단계에서는 LLM이 인간의 의도에 맞게 '정렬(alignment)' 되어 있지 않다고 말한다. 정렬은 LLM에서 매우 중요한 주제이며, 이 문제를 어느 정도 해결할 수 있는 방법도 곧 배우게 될 것이다. 실제로, 사전 학습만 된 LLM도 어느 정도는 조정이 가능하기 때문에, 처음에는 지시를 잘 따르지 못하더라도 나중에는 그렇게 하도록 가르칠 수 있다.

### 명령어 미세조정(Instruction fine-tuning)과 RLHF
여기서 명령어 미세조정(instruction tuning)이 등장한다. 사전 학습이 끝난 LLM을 가져와, 이번에는 고품질의 명령어와 답변 쌍만을 사용해(즉, 이전과 마찬가지로 한 번에 한 단어씩 예측하는 방식으로) 다시 학습을 시킨다.

이렇게 하면, 모델은 단순히 텍스트를 이어 쓰는 역할에서 벗어나, 사용자의 의도에 맞게 지시를 따르고 도움이 되는 답변을 하는 '도우미(assistant)'로서의 역할을 배우게 된다. 이때 사용하는 명령어-답변 데이터셋은 보통 사전 학습 데이터셋보다 훨씩 적다. 왜냐하면, 이런 고품질 데이터는 대부분 사람이 직접 만들어야 하므로 비용이 많이 들기 때문이다. 이는 사전 학습에서 사용한 저렴한 자기지도 데이터와는 매우 다르다. 그래서 이 단계를 '지도 명령어 미세조정(supervised instruction fine-tuning)'이라고도 부른다.

또한, ChatGPT와 같은 일부 LLM은 세 번째 단계인 '인간 피드백을 활용한 강화학습(RLHF, Reinforcement Learning from Human Feedback)'도 거친다. 여기서는 자세히 다루지 않겠지만, 목적은 명령어 미세조정과 비슷하다. RLHF 역시 모델이 인간의 가치와 선호에 맞는 답변을 하도록 도와준다. 초기 연구에 따르면, 이 단계가 인간 수준의 성능에 도달하거나 이를 뛰어넘는데 매우 중요하다고 한다. 실제로, 강화학습과 언어모델링을 결합하는 연구가 매우 유망하게 여겨지며, 앞으로 LLM의 성능이 크게 향상될 것으로 기대된다.

이제 몇 가지 대표적인 활용 사례를 통해 우리가 이해한 내용을 점검해보자.

> [!Note] RLHF(인간 피드백을 활용한 강화 학습)란?
> - **강화학습이란?**  
>     강화학습은 “잘한 행동에는 보상, 잘못된 행동에는 벌점”을 주며, AI가 스스로 더 좋은 행동을 찾도록 하는 기계학습 방법입니다. 마치 게임에서 점수를 올리려 여러 번 시도하는 것과 비슷합니다.
> - **RLHF란?**  
>     RLHF(Reinforcement Learning from Human Feedback)는 ‘인간의 평가’를 보상 신호로 삼아 AI를 훈련시키는 방식입니다.  
>     예를 들어, ChatGPT의 답변에 대해 사람이 “이 답변이 더 좋다/별로다”라고 평가해주면, 모델은 더 좋은 답변을 내기 위해 학습합니다.
> - **ChatGPT에서의 예시**  
>     ChatGPT를 개발할 때, 여러 답변 후보를 생성한 뒤, 실제 사람이 “이 답변이 더 유익하다/더 친절하다/더 정확하다” 등으로 순위를 매깁니다.  
>     모델은 이런 인간의 평가(피드백)를 바탕으로, 더 좋은 답변을 내놓을수록 더 높은 보상을 받도록 훈련됩니다.  
>     우리가 ChatGPT를 쓸 때 “이 답변이 도움이 되었나요?”라는 질문에 ‘예/아니오’로 답하는 것도 이런 피드백 데이터의 일부가 됩니다.
> - **정리**  
>     RLHF는 AI가 인간의 가치와 선호에 더 잘 맞는 답변을 하도록, 인간의 평가를 활용해 추가로 훈련시키는 단계입니다.

![[Pasted image 20250803204338.png]]

첫쨰, 왜 LLM은 긴 글의 요약을 잘할 수 있을까? (혹시 몰랐다면, 정말 잘한다. 문서를 붙여넣고 요약해 달라고 요청해봐라)

이유를 이해하려면, 학습 데이터를 떠올려야 한다. 실제로 사람들은 인터넷, 논문, 책 등에서 요약을 자주 만든다. 따라서 이런 데이터를 학습한 LLM도 자연스럽게 요약하는 방법을 배우게 된다. 즉, 핵심 내용을 파악하고 짧은 글로 압축하는 법을 익히는 것이다.

요약을 생헝할 때, 전체 글이 LLM의 입력 시퀀스에 포함된다는 점도 주목해야 한다. 예를 들어, 논문에서 본문 다음에 결론이 나오는 구조와 비슷하다.

결과적으로, 이런 요약 능력은 이미 사전 학습 단계에서 습득되었을 가능성이 높고, 명령어 미세조정 단계에서 그 능력이 더욱 향상되었을 것이다. 이 단계에서도 요약 예시가 일부 포함되었을 것으로 불 수 있다.

둘째, 왜 LLM은 상식적인 질문에 답할 수 있을까?

앞서 언급했듯이, 도우미 역할을 하며 적절히 답변하는 능력은 명령어 미세조정과 RLHF 덕분이다. 하지만 질문에 답하기 위한 지식 자체(혹은 대부분)는 이미 사전 학습 단계에서 습득된 것이다.

물론, 여기서 또 다른 중요한 질문이 생긴다. 만약 LLM이 답을 모르면 어떻게 될까? 안타깝게도, 그럴 경우 그냥 답을 지어낼 수도 있다. 왜 그런지 이해하려면, 다시 한 번 학습 데이터와 학습 목표를 생각해봐야 한다.

![[Pasted image 20250803205003.png]]

LLM과 관련해 "환각(hallucination)"이라는 용어를 들어본 적이 있을 텐데, 이는 LLM이 사실이 아닌 내용을 지어내는 현상을 의미한다.

왜 이런 일이 발생할까? LLM은 단지 텍스트를 생성하는 법만 배웠지, 사실에 기반한 진짜 정보를 생성하는 법을 배우지 않았기 때문이다. 학습 과정에서 데이터의 진실성이나 신뢰도를 판단할 수 있는 단서는 전혀 주어지지 않았다. 하지만 이것이 전부는 아니다. 인터넷이나 책에 있는 그를은 대체로 자신감 있게 쓰여 있기 때문에, LLM도 틀린 내용이라도 자신감 있게 말하는 법을 배우게 된다. 이런 식으로, LLM은 불확실성을 거의 드러내지 않는다.

이 문제는 현재 활발히 연구되고 있는 분야이며, 앞으로 시간이 지나면서 LLM의 환각 현상이 점점 줄어들 것으로 기대할 수 있다. 예를 들어, 명령어 미세조정 단계에서 LLM이 환각을 자제하도록 어느 정도 가르칠 수 있지만, 이 문제가 완전히 해결될지는 두고 봐야 한다.

놀랍게도, 우리는 이 문제를 부분적으로라도 해결할 수 있는 방법을 이미 알고 있고, 실제로 널리 사용되고 있다.

![[Pasted image 20250803205309.png]]

LLM에게 "콜롬비아의 현 대통령은 누구인가요?"라고 물어본다고 해보자. LLM이 잘못된 이름을 답할 가능성이 높다. 그 이유는 두 가지다.

첫째, 앞서 언급한 것처럼 LLM이 환각(hallucination) 현상으로 잘못된 이름이나 심지어 가짜 이름을 답할 수 있다.
둘째, LLM은 특성 시점까지의 데이터만으로 학습되기 때문에(예를 들어 작년까지의 데이터), 그 이후에 대통령이 바뀌었을 경우 최신 정보를 알 수 없다.

그렇다면 이 두가지 문제를 어떻게 해결할 수 있을까? 답은 모델에게 관련된 '맥락(context)'을 제공하는 데 있다. 여기서 중요한 점은, LLM의 입력 시퀀스에 포함된 정보는 모델이 바로 활용할 수 있지만, 사전 학습에서 얻은 암묵적인 지식은 꺼내 쓰기가 어렵고 불확실하다는 것이다.

예를 들어, 콜롬비아의 정치사에 관한 위키피디아 문서를 LLM에 함께 입력해준다면(그리고 그 문서가 최신 정보라면), LLM은 그 맥락에서 대통령의 이름을 뽑아내어 훨씬 정확하게 답할 수 있다.

위의 이미지는 추가 맥락이 포함된 LLM 프롬프트(입력 예시)가 어떻게 생겼는지 보여준다. (참고로, 프롬프트란 LLM에게 주는 지시문, 즉 입력 시퀀스를 의미한다.)

이 과정을 'LLM을 맥락에 기반해 작동하게 한다(grounding the LLM in the context)'고 부른다. 즉, LLM이 자유롭게 아무 답이나 생성하는 것이 아니라, 실제 세계의 정보를 바탕으로 답하게 만드는 것이다.

이것이 바로 Bing Chat 등 검색 기반 LLM이 작동하는 방식이다. 먼저 검색 엔진을 통해 웹에서 관련 맥락을 추출한 뒤, 그 정보를 사용자의 질문과 함께 LLM에 전달한다. 위 그림을 보면 이 과정이 어떻게 이루어지는 시각적으로 알 수 있다.

![[Pasted image 20250803210144.png]]

이제 우리는 최신 LLM(2023년 하반기 기준)의 주요 작동 원리를 거의 이해한 상태에 도달했다.

"사실 별로 신기하지 않은데? 결국 한 번에 한 단어씩 예측하는 것뿐이고, 결국 통계일 뿐이잖아?"라고 생각할 수도 있다.

조금만 다시 생각해보자. 이 모든 것의 신기한 점은, 정말 놀라울 정도로 잘 작동한다는데 있다. 실제로 OpenAI의 연구자들조차 이런 언어 모델링이 이렇게까지 잘 될 줄은 예상하지 못했다. 최근 몇 년간 성능이 크게 향상된 주요 원인 중 하나는 신경망과 데이터셋의 규머가 엄청나게 커졌기 때문이다. 예를 들어, GPT-4는 1조 개가 넘는 파라미터를 가진 모델로, 변호사 시험이나 AP 생물 시험에서 상위 10%에 드는 점수를 받을 수 있다고 알려져 있다.

놀랍게도, 이런 대형 LLM들은 명시적으로 훈련받지 않은 작업도 해결하는 '새로운 능력(emerging abilities)'을 보이기도 한다.

이 글의 마지막 부분에서는 이런 새로운 능력 중 일부를 살펴보고, 실제로 문제 해결에 활용할 수 있는 볓 가지 팁도 소개하겠다.

![[Pasted image 20250803210525.png]]

LLM의 대표적인 새로운 능력 중 하나는, 이름 그대로 '한 번도 훈련받지 않은 완전히 새로운 작업'을 수행할 수 있다는 점이다. 이를 '제로샷(zero-shot)'이라고 부른다. 필요한 것은 단지 "이 작업을 이렇게 해달라"는 지시뿐이다.

이 능력을 재미있는 예시로 설명해보면, LLM에게 "독일어 문장을 영어로 번역하되, 모든 단어가 f로 시작하게 해줘"라고 요청할 수 있다.

실제로, “Die Katze schläft gerne in der Box”(독일어로 “고양이는 상자에서 자는 것을 좋아한다”는 뜻)을 f로 시작하는 단어만 써서 번역해보라고 했더니, LLM은 “Feline friend finds fluffy fortress”(고양이 친구가 푹신한 요새를 찾는다)라고 번역했다.

![[Pasted image 20250803210801.png]]

더 복잡한 작업의 경우, 제로샷 프롬프트(설명만으로 시도하는 것)로는 매우 자세한 지시가 필요하다는 점을 금방 알게 될 것이고, 그럼에도 불구하고 성능이 완벽과는 거리가 멀다는 것도 느끼게 된다.

이 부분은 인간의 지능과도 연결된다. 누군가 여러분에게 새로운 일을 시키면, 아마도 “어떻게 하는지 예시나 시범을 보여달라”고 요청할 것이다. LLM도 마찬가지로, 예시를 주면 더 잘 작동한다.

예를 들어, 여러 통화 단위를 공통된 형식으로 번역하도록 모델에 요청한다고 해보자. 원하는 바를 자세히 설명할 수도 있고, 간단한 지시와 함께 몇 가지 예시를 줄 수도 있다. 위의 이미지는 이런 작업의 예시를 보여준다.

이런 프롬프트를 사용하면, 마지막 예시인 “Steak: 24.99 USD”에 대해 모델이 $24.99라고 잘 답할 수 있다.

여기서 마지막 예시의 정답은 일부러 비워두었다. LLM은 본질적으로 텍스트를 이어 쓰는(완성하는) 모델이기 때문에, 구조를 일관되게 유지하는 것이 중요다. 위 예시처럼, 원하는 답변만 나오도록 유도하는 것이 좋다.

정리하자면, LLM이 제로샷 방식에서 어려움을 겪는다면, 몇 가지 예시를 함께 제공하는 것이 일반적인 팁이다. 이렇게 하면 LLM이 작업을 더 잘 이해하고, 성능도 더 좋아지고 신뢰성도 높아진다.

![[Pasted image 20250803211030.png]]

LLM의 또 다른 흥미로운 능력은 인간의 지능을 떠올리게 한다는 점이다. 이 능력은 특히 더 복잡하고 여러 단계의 추론이 필요한 작업에서 유용다.

예를 들어, “리오넬 메시가 태어나기 전 해에 월드컵에서 우승한 나라는 어디인가요?”라고 묻는다고 해보자. 여러분은 아마도 정답에 도달하기 위해 필요한 중간 단계들을 하나씩 적으면서 차근차근 문제를 풀 것이다. LLM도 바로 이런 식으로 문제를 해결할 수 있다.

실제로 LLM에게 “step by step(단계별로 생각해봐)”라고 지시하는 것만으로도, 많은 작업에서 성능이 크게 향상된다는 것이 밝혀졌다.

왜 이런 방식이 효과적일까요? 그 이유는 이미 우리가 앞에서 배운 내용에 있다. 이런 복합적인 지식은 LLM의 내부 기억에 바로 저장되어 있지 않을 수 있지만, 메시의 생일이나 각 월드컵 우승국처럼 개별적인 사실들은 기억하고 있을 수 있다.

LLM이 최종 답에 도달하기까지 중간 단계를 차근차근 쌓아가게 하면, 마치 ‘작업 기억(working memory)’처럼 모델이 생각을 정리할 시간을 갖게 되고, 최종 답을 내기 전에 더 단순한 하위 문제들을 먼저 해결할 수 있다.

여기서 중요한 점은, LLM이 새로 단어를 생성할 때마다 그 왼쪽에 있는 모든 내용이 모델이 참고할 수 있는 ‘맥락’이 된다는 것이다. 그래서 위 그림처럼, 모델이 “아르헨티나”라고 답할 때쯤에는 이미 메시의 생일과 우리가 물어본 월드컵 연도가 LLM의 작업 기억에 들어가 있어, 더 쉽게 정답을 낼 수 있게 된다.

## 결론
마무리하기 전에, 글 앞부분에서 던졌던 질문에 답하고 싶다. LLM은 정말로 단순히 다음 단어만 예측하는 것일까, 아니면 그 이상일까? 일부 연구자들은 후자라고 주장다. 즉, 어떤 맥락에서든 다음 단어를 그렇게 잘 예측하려면, LLM이 내부적으로 세상에 대한 압축된 이해를 갖추고 있어야 한다는 것다. 반면, 다른 연구자들은 LLM이 단지 훈련 중에 본 패턴을 암기하고 복사하는 법만 배웠을 뿐, 언어나 세상, 그 어떤 것에 대한 진짜 이해는 없다고 주장다.

아마도 이 두 입장 중 어느 쪽이 옳다/그르다를 지금 단정할 수는 없을 것이다. 어쩌면 같은 현상을 바라보는 서로 다른 시각일 수도 있다. 분명한 것은, LLM이 매우 유용하며, 놀라운 지식과 추론 능력을 보여주고 있다는 점다. 어쩌면 일반 지능의 조짐도 보일 수 있다. 하지만 이것이 인간의 지능과 얼마나 닮았는지, 그리고 언어 모델링이 앞으로 얼마나 더 발전할 수 있을지는 아직 미지수다.

이 글이 LLM과 최근의 AI 열풍을 이해하는 데 도움이 되었기를 바다. 그래야 AI의 잠재력과 위험성에 대해 여러분 스스로 의견을 가질 수 있을 것다. AI가 세상에 어떻게 쓰일지 결정하는 것은 AI 연구자나 데이터 과학자만의 몫이 아다. 모두가 의견을 낼 수 있어야 하기에, 배경지식이 많지 않아도 읽을 수 있는 글을 쓰고 싶었다.

이 글을 끝까지 읽으셨다면, 2023년 가을 기준 최신 LLM이 어떻게 작동하는지(적어도 큰 틀에서는) 충분히 이해으리라 생각한다.

마지막으로 인공지능과 LLM의 현재 상태에 대한 내 생각을 말해주겠다.

![[Pasted image 20250803211440.png]]

