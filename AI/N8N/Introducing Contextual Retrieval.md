특정 컨텍스트에 도움이 되는 AI 모델을 만들기 위해서, 종종 배경 지식에 접근하는 것이 요구된다. 예를들어, 고객 지원 챗봇이 특정 비지니스에 대한 지식을 필요로 하고, 법률 분석 봇은 방대한 과거 사례에 대한 지식이 필요하다.

개발자들은 주로 AI 모델의 지식을 Retrival-Augmented Generation(RAG)를 사용해서 향상시킨다. RAG는 지식 체계로부터 연관된 정보를 수집하는 방법이며, 모델의 응답 품질을 크게 향상시켜 준다. 문제는 전통적인 RAG 솔루션은 정보를 인코딩 할 때 맥락을 제거하며, 종종 연관된 정보 수집에 실패하게 만든다.

이번 포스트에서, 우리는 RAG의 검색 단계를 크게 개선하는 방법을 소개한다. 이 방법은 "Contextual Retrieval"이라고 하며, 두 가지 하위 기술인 Contextual Embeddings와 Contextual BM25를 사용한다. 이 방법을 사용하면 검색 실패 횟수를 49% 줄일 수 있고, 리랭킹과 결합하면 67% 까지 줄일 수 있다. 이는 검색 정확도의 상당한 향상을 의미하며, 곧 후속 작업의 성능 향상으로 직접 이어진다.

## 단순하게 방대한 프롬프트를 사용하는 방법
종종 가장 단순한 방식이 최선의 방식이다. 만약 지식 기반이 200,000 토큰(500 페이지 분량) 보다 작다면, RAG 또는 비슷한 방식 없이 그냥 전체 지식 기반을 프롬프트에 넣을수도 있다.

## RAG에 대한 기초: 더 큰 지식 베이스로 확장하기
컨텍스트 윈도우에 담을 수 없는 거대 지식 베이스를 위해 RAG는 전형적인 솔루션이다. RAG는 다음의 단계를 따라 지식 베이스를 전처리 함으로써 동작한다:
1. 지식 베이스(문서 집합)를 더 작은 텍스트 조각으로 분할한다, 보통 몇백 토큰을 넘지 않게 나눈다.
2. 임베딩 모델을 사용해 이 조각들을 의미를 담은 벡터 임베딩으로 변환한다.
3. 이 임베딩을 벡터 데이터베이스에 저장하면 의미적 유사도를 기반으로 검색할 수 있게 된다.

런타임에, 유저가 모델에 질문을 하면, 의미적 유사도에 기반해 가장 연관된 청크를 찾는데 벡터 데이터베이스가 사용된다. 이후, 가장 유사한 청크가 생성형 모델의 프롬프트에 추가된다.

임베딩 모델은 의미적 관계를 잘 포착하지만, 중요한 정확 일치를 놓칠 수 있다. 다행히 이런 상황에서 도움이 되는 오래된 기법이 있다. BM25(Best Matching 25)는 어휘적 일치를 사용해 정확한 단어나 구를 찾아내는 랭킹 함수다. 특히 고유 식별자나 기술 용어가 포함된 쿼리에 효과적이다.

BM25는 TF-IDF(단어 빈도-역문서 빈도) 개념을 기반으로 동작한다. TF-IDF는 한 단어가 문서 집합 내에서 특정 문서에 얼마나 중요한지를 측정한다. BM25는 여기에 문서 길이를 고려하고, 단어 빈도에 포화 함수를 적용해 자주 등장하는 일반 단어가 결과를 지배하지 않도록 보완한다.

의미적 임베딩이 실패하는 상황에서 BM25가 도움이 되는 상황은 다음과 같다: 사용자가 기술 지원 데이터베이스에서 "Error code TS-999"를 검색한다고 가정해보자. 임베딩 모델은 일반적인 오류 코드에 대한 내용을 찾을 수 있지만, 정확히 "TS-999" 라는 일치를 놓칠 수 있다. BM25는 이 특정한 텍스트 문자열을 찾아 관련 문서를 식별한다.

RAG 솔루션은 임베딩과 BM25 기법을 결합해 다음 단계들을 통해 가장 적합한 조각을 더 정확하게 검색할 수 있다:
1. 지식 베이스(문저 집합)를 더 작은 텍스트 조각으로 분할한다, 보통 몇백 토큰을 넘지 않게 나눈다.
2. TF-IDF 인코딩을 생성하고 이 청크를 위한 의미적 임베딩을 추가한다.
3. BM25를 사용해 정확히 일치하는 상위 조각들을 찾는다.
4. 임베딩을 사용해 의미적 유사도를 기반으로 청크를 찾는다.
5. 랭크 퓨전(rank fusion) 기법을 사용해 (3)과 (4)에서 나온 결과를 결합하고, 중복되는 항목을 제거한다.
6. top-K 청크를 프롬프트에 추가해 응답을 생성한다.

> [!Note] top-K
> K는 숫자를 의미하며, 예를들어 K=5라면 상위 5개의 조각을 의미한다. 즉, 검색 결과 중에서 가장 중요한 K개를 골라서 AI에게 제공한다는 뜻이다.

BM25와 임베딩 모델을 모두 활용함으로써, 기존 RAG 시스템은 정확한 용어 일치와 넓은 의미 이해를 균형 있게 결합해 더 포괄적이고 정확한 결과를 제공할 수 있다.

![[Pasted image 20250726215335.png]]

이 접근법을 사용하면 하나의 프롬프트에 담을 수 있는 범위를 넘어서는 방대한 지식 베이스로 비용 효율적으로 확장할 수 있다. 하지만 이러한 기존 RAG 시스템에는 중요한 한계가 있는데, 바로 맥락(context)을 자주 손상시킨다는 점이다.

## 기존 RAG에서의 맥락 문제
전통적인 RAG에서, 문서들은 보통 효율적인 수집을 위해 더 작은 청크들로 분할된다. 대부분의 어플리케이션에서 이 접근 방식이 유효하게 동작하지만, 각 청크에서 충분한 맥락이 없을 떄는 문제가 발생할 수 있다.

예를들어, 지식 베이스에 임베드된 재무 정보가 있다고 가정해보자, 그리고 아래와 같은 질문을 받았다: "2023년 2분기의 ACME Corp의 수익 성장률을 얼마인가요?"

유사한 청크들은 다음과 같은 텍스트를 포함할 수 있다: "회사의 수익률을 이전 분기보다 3% 성장했다" 하지만 이 청크는 어떤 회사에 대한 정보인지나, 기간에 대한 정보가 누락되어 있다. 그로 인해 정확한 정보를 효과적으로 사용하거 어렵게 만든다.

## Contextual Retrieval에 대한 소개


#n8n 