특정 컨텍스트에 도움이 되는 AI 모델을 만들기 위해서, 종종 배경 지식에 접근하는 것이 요구된다. 예를들어, 고객 지원 챗봇이 특정 비지니스에 대한 지식을 필요로 하고, 법률 분석 봇은 방대한 과거 사례에 대한 지식이 필요하다.

개발자들은 주로 AI 모델의 지식을 Retrival-Augmented Generation(RAG)를 사용해서 향상시킨다. RAG는 지식 체계로부터 연관된 정보를 수집하는 방법이며, 모델의 응답 품질을 크게 향상시켜 준다. 문제는 전통적인 RAG 솔루션은 정보를 인코딩 할 때 맥락을 제거하며, 종종 연관된 정보 수집에 실패하게 만든다.

이번 포스트에서, 우리는 RAG의 검색 단계를 크게 개선하는 방법을 소개한다. 이 방법은 "Contextual Retrieval"이라고 하며, 두 가지 하위 기술인 Contextual Embeddings와 Contextual BM25를 사용한다. 이 방법을 사용하면 검색 실패 횟수를 49% 줄일 수 있고, 리랭킹과 결합하면 67% 까지 줄일 수 있다. 이는 검색 정확도의 상당한 향상을 의미하며, 곧 후속 작업의 성능 향상으로 직접 이어진다.

## 단순하게 방대한 프롬프트를 사용하는 방법
종종 가장 단순한 방식이 최선의 방식이다. 만약 지식 기반이 200,000 토큰(500 페이지 분량) 보다 작다면, RAG 또는 비슷한 방식 없이 그냥 전체 지식 기반을 프롬프트에 넣을수도 있다.

## RAG에 대한 기초: 더 큰 지식 베이스로 확장하기
컨텍스트 윈도우에 담을 수 없는 거대 지식 베이스를 위해 RAG는 전형적인 솔루션이다. RAG는 다음의 단계를 따라 지식 베이스를 전처리 함으로써 동작한다:
1. 지식 베이스(문서 집합)를 더 작은 텍스트 조각으로 분할한다, 보통 몇백 토큰을 넘지 않게 나눈다.
2. 임베딩 모델을 사용해 이 조각들을 의미를 담은 벡터 임베딩으로 변환한다.
3. 이 임베딩을 벡터 데이터베이스에 저장하면 의미적 유사도를 기반으로 검색할 수 있게 된다.

런타임에, 유저가 모델에 질문을 하면, 의미적 유사도에 기반해 가장 연관된 청크를 찾는데 벡터 데이터베이스가 사용된다. 이후, 가장 유사한 청크가 생성형 모델의 프롬프트에 추가된다.

임베딩 모델은 의

#n8n 