특정 컨텍스트에 도움이 되는 AI 모델을 만들기 위해서, 종종 배경 지식에 접근하는 것이 요구된다. 예를들어, 고객 지원 챗봇이 특정 비지니스에 대한 지식을 필요로 하고, 법률 분석 봇은 방대한 과거 사례에 대한 지식이 필요하다.

개발자들은 주로 AI 모델의 지식을 Retrival-Augmented Generation(RAG)를 사용해서 향상시킨다. RAG는 지식 체계로부터 연관된 정보를 검ㅅㄱ하는 방법이며, 모델의 응답 품질을 크게 향상시켜 준다. 문제는 전통적인 RAG 솔루션은 정보를 인코딩 할 때 맥락을 제거하며, 종종 연관된 정보 검색에 실패하게 만든다.

이번 포스트에서, 우리는 RAG의 검색 단계를 크게 개선하는 방법을 소개한다. 이 방법은 "Contextual Retrieval"이라고 하며, 두 가지 하위 기술인 Contextual Embeddings와 Contextual BM25를 사용한다. 이 방법을 사용하면 검색 실패 횟수를 49% 줄일 수 있고, 리랭킹과 결합하면 67% 까지 줄일 수 있다. 이는 검색 정확도의 상당한 향상을 의미하며, 곧 후속 작업의 성능 향상으로 직접 이어진다.

## 단순하게 방대한 프롬프트를 사용하는 방법
종종 가장 단순한 방식이 최선의 방식이다. 만약 지식 기반이 200,000 토큰(500 페이지 분량) 보다 작다면, RAG 또는 비슷한 방식 없이 그냥 전체 지식 기반을 프롬프트에 넣을수도 있다.

## RAG에 대한 기초: 더 큰 지식 베이스로 확장하기
컨텍스트 윈도우에 담을 수 없는 거대 지식 베이스를 위해 RAG는 전형적인 솔루션이다. RAG는 다음의 단계를 따라 지식 베이스를 전처리 함으로써 동작한다:
1. 지식 베이스(문서 집합)를 더 작은 텍스트 조각으로 분할한다, 보통 몇백 토큰을 넘지 않게 나눈다.
2. 임베딩 모델을 사용해 이 조각들을 의미를 담은 벡터 임베딩으로 변환한다.
3. 이 임베딩을 벡터 데이터베이스에 저장하면 의미적 유사도를 기반으로 검색할 수 있게 된다.

런타임에, 유저가 모델에 질문을 하면, 의미적 유사도에 기반해 가장 연관된 청크를 찾는데 벡터 데이터베이스가 사용된다. 이후, 가장 유사한 청크가 생성형 모델의 프롬프트에 추가된다.

임베딩 모델은 의미적 관계를 잘 포착하지만, 중요한 정확 일치를 놓칠 수 있다. 다행히 이런 상황에서 도움이 되는 오래된 기법이 있다. BM25(Best Matching 25)는 어휘적 일치를 사용해 정확한 단어나 구를 찾아내는 랭킹 함수다. 특히 고유 식별자나 기술 용어가 포함된 쿼리에 효과적이다.

BM25는 TF-IDF(단어 빈도-역문서 빈도) 개념을 기반으로 동작한다. TF-IDF는 한 단어가 문서 집합 내에서 특정 문서에 얼마나 중요한지를 측정한다. BM25는 여기에 문서 길이를 고려하고, 단어 빈도에 포화 함수를 적용해 자주 등장하는 일반 단어가 결과를 지배하지 않도록 보완한다.

의미적 임베딩이 실패하는 상황에서 BM25가 도움이 되는 상황은 다음과 같다: 사용자가 기술 지원 데이터베이스에서 "Error code TS-999"를 검색한다고 가정해보자. 임베딩 모델은 일반적인 오류 코드에 대한 내용을 찾을 수 있지만, 정확히 "TS-999" 라는 일치를 놓칠 수 있다. BM25는 이 특정한 텍스트 문자열을 찾아 관련 문서를 식별한다.

RAG 솔루션은 임베딩과 BM25 기법을 결합해 다음 단계들을 통해 가장 적합한 조각을 더 정확하게 검색할 수 있다:
1. 지식 베이스(문저 집합)를 더 작은 텍스트 조각으로 분할한다, 보통 몇백 토큰을 넘지 않게 나눈다.
2. TF-IDF 인코딩을 생성하고 이 청크를 위한 의미적 임베딩을 추가한다.
3. BM25를 사용해 정확히 일치하는 상위 조각들을 찾는다.
4. 임베딩을 사용해 의미적 유사도를 기반으로 청크를 찾는다.
5. 랭크 퓨전(rank fusion) 기법을 사용해 (3)과 (4)에서 나온 결과를 결합하고, 중복되는 항목을 제거한다.
6. top-K 청크를 프롬프트에 추가해 응답을 생성한다.

> [!Note] top-K
> K는 숫자를 의미하며, 예를들어 K=5라면 상위 5개의 조각을 의미한다. 즉, 검색 결과 중에서 가장 중요한 K개를 골라서 AI에게 제공한다는 뜻이다.

BM25와 임베딩 모델을 모두 활용함으로써, 기존 RAG 시스템은 정확한 용어 일치와 넓은 의미 이해를 균형 있게 결합해 더 포괄적이고 정확한 결과를 제공할 수 있다.

![[Pasted image 20250726215335.png]]

이 접근법을 사용하면 하나의 프롬프트에 담을 수 있는 범위를 넘어서는 방대한 지식 베이스로 비용 효율적으로 확장할 수 있다. 하지만 이러한 기존 RAG 시스템에는 중요한 한계가 있는데, 바로 맥락(context)을 자주 손상시킨다는 점이다.

## 기존 RAG에서의 맥락 문제
전통적인 RAG에서, 문서들은 보통 효율적인 검색을 위해 더 작은 청크들로 분할된다. 대부분의 어플리케이션에서 이 접근 방식이 유효하게 동작하지만, 각 청크에서 충분한 맥락이 없을 떄는 문제가 발생할 수 있다.

예를들어, 지식 베이스에 임베드된 재무 정보가 있다고 가정해보자, 그리고 아래와 같은 질문을 받았다: "2023년 2분기의 ACME Corp의 수익 성장률을 얼마인가요?"

유사한 청크들은 다음과 같은 텍스트를 포함할 수 있다: "회사의 수익률을 이전 분기보다 3% 성장했다" 하지만 이 청크는 어떤 회사에 대한 정보인지나, 기간에 대한 정보가 누락되어 있다. 그로 인해 정확한 정보를 효과적으로 사용하거 어렵게 만든다.

## Contextual Retrieval에 대한 소개
Contextual Retrieval은 BM25 인덱스(Contextual BM25)를 생성하고 임베딩(Contextual Embeddings)을 생성하기 전에 각 청크에 특정한 설명 컨텍스트를 추가해 이러한 문제를 해결한다.

아래는 그 예시다:
```text
원본_청크 = "회사의 수익 성장률은 지난 분기 대비 3%다."

맥락이_추가된_청크="이 정크는 ACME 회사의 2023년 2분기 SEC 파일링으로 부터 왔습니다; 이전 분기의 수익률은 3억 1400만 달러 입니다. 회사의 수익 성장률은 이전 분기 대비 3% 입니다."
```

과거에도 맥락을 사용해 검색 성능을 높이려는 여러 방법이 제안된 바 있다. 예를 들어, 각 조각에 일반적인 문서 요약을 추가하는 방법(실험 결과 효과가 매우 제한적이었음), 가상 문서 임베딩, 요약 기반 인덱싱(평가 결과 성능이 낮았음) 등이 있다. 이 글에서 제안하는 방법은 이러한 기존 방법들과는 다르다.

## Contextual Retrieval 구현하기
물론, 지식 베이스에 있는 수천 또는 수백만 개의 조각에 일일이 수작업으로 주석을 다는 것은 너무 많은 작업이 될것이다. 그래서 Contextual Retrieval을 구현하기 위해 Claude를 활용한다. 우리는 모델에게 전체 문서의 맥락을 사용해 각 조각을 설명하는 간결하고ㅗ 조각별 맥락을 제공하도록 지시하는 프롬프트를 작성했다. 각 조각의 맥락을 생성하기 위해 아래의 Claude 3 Haliku 프롬프트를 사용했다.

```text
<document> 
{{WHOLE_DOCUMENT}} 
</document> 
Here is the chunk we want to situate within the whole document 
<chunk> 
{{CHUNK_CONTENT}} 
</chunk> 
Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else. 
```

생성된 맥락 정보 텍스트는 주로 50-100 토큰이며, BM25 인덱스가 생성되기 전에 추가된다. 

아래는 전처리 과정이 실제로 동작 하는 방식이다:
![[Pasted image 20250726230153.png]]

Contextual Retrieval에 관심이 있다면, [our cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main/skills/contextual-embeddings)에서 확인할 수 있다.

## Contextual Retrieval의 코스트를 줄이기 위해 프롬프트 캐싱하기
위에서 언급한 특별한 프롬프트 캐싱 기능 덕분에, Claude를 사용하면 Contextual Retrieval을 매우 저렴하게 구현할 수 있다. 프롬프트 캐싱을 사용하면, 각 조각마다 참조 문서를 매번 전달할 필요가 없다. 문서를 한 번 캐시에 올려두고, 이후에는 이미 저장된 내용을 참조하면 된다. 예를 들어, 800 토큰짜리 조각, 8000 토큰짜리 문서, 50 토큰짜리 맥락 지시문, 조각당 100토큰의 맥락을 가정할 때, Contextual 조각을 생성하는 데 드는 일회성 비용은 문서 토큰 100만 개당 1.02 달러다.

### 방법론
우리는 다양한 지식 분야(코드베이스, 소설, Arxiv 논문, 과학 논문), 인베딩 모델, 검색 전략, 평가 지표를 대상으로 실험을 진행했다. 각 분야에서 사용한 질문과 답변의 예시는 [부록2](https://assets.anthropic.com/m/1632cded0a125333/original/Contextual-Retrieval-Appendix-2.pdf) 에 포함되어 있다.

아래 그래프는 최고의 임베딩 설정(Gemini Text 004)과 상위 20개 조각을 검색했을 때, 모든 지식 분야에서의 평균 성능을 보여준다. 우리는 평가 지표로 1-리콜@20(상위 20개 조각 내에서 검색되지 않은 관련 문서의 비율)을 사용했다. 전체 결과는 부록에서 확인할 수 있으며, 맥락화는 우리가 평가한 모든 임베딩-소스 조합에서 성능을 향상시켰다.

### 성능 향상
우리의 실험은 아래를 보여준다:
- **맥락 임베딩은 top-20-청크 검색집 실패 확률을 35% 줄여준다(5.7% -> 3.7%).**
- **맥락 임베딩과 Contextual BM25를 함께 사용하면 top-20-청크 검색 실패 확률을 49% 줄여준다(5.7% -> 2.9%).

![[Pasted image 20250726232640.png]]

### 구현시 고려사항
Contextual Retrieval을 구현할 때 몇 가지 고려사항이 있다:
1. **청크 경계**: 어떻게 문서를 청크로 나눌 것인지 고려해야 한다. 청크 사이즈, 청크 경계, 청크 오버랩은 검색 성능에 형향을 줄 수 있다.
2. **임베딩 모델**: 


#n8n 